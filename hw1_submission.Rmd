---
title: "Intro to Causal Inference HW1"
author: "Julian McClellan"
date: "February 13, 2018"
output: pdf_document
---

```{r data_read_transform, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
theme_set(theme_minimal())
# dat <- read_dta("K_ELL_T2LEARN_exercise.dta")
og_dat <- read_csv("GSS83_91.csv")

dat <- read_csv("GSS83_91.csv") %>%
  filter(!(tvhours %in% c(-1, 98, 88)),
         !(wrkstat %in% c(0, 9)),
         attend != 9,
         (wrkstat != 0 & wrkstat < 8),
         age >= 25,
         age < 65,
         (tvhours != -1 & tvhours < 98),
         attend < 9,
         educ < 97
         ) %>%
  mutate(wd = ifelse(wrkstat >= 3 & wrkstat <= 8, 1, 0),
         race_d = ifelse(race == 2, 1, 0),
         atddmy = ifelse(attend >= 4 & attend <= 8, 1, 0),
         educ5 = educ, 
         educ5 = case_when(
           (educ5 >= 0 & educ5 <= 11) ~ 1,
           educ5 == 12 ~ 2,
           (educ5 >= 13 & educ5 <= 15) ~ 3,
           educ5 == 16 ~ 4,
           (educ5 >= 17 & educ5 <= 20) ~ 5),
         age10c = floor((age - 5) / 10),
         atddmy = factor(atddmy, levels = c(0, 1), labels = c(
           "Attends church less than once a month",
           "Attends church at least once a month"
         )),
         
         # INteraction
         int = wd * ifelse((age10c >= 3 & age10c <= 4), 1, 0),
         educ5 = factor(educ5, levels = c(2, 1, 3, 4, 5)),
         age10c = factor(age10c, levels = c(2, 3, 4, 5))
         )


# Make variables necessary
```

### 1. Run the logistic regression to predict the treatment variable ATDDMY including AGE10C, EDUC5, RACE_D, WD, and INT as covariates. Treat AGE10C and EDUC5 as categorical variables by using the first category (25-34) as the baseline category for age, and the second category (high school graduation) as the baseline category for education.

```{r 1}
q_1_logit <- glm(atddmy ~ age10c + educ5 + race_d + wd + int, data = dat, family = binomial)

odds <- exp(coef(q_1_logit))
```

### 2. Describe the effects of (1) race dummy variable, (2) the effect of employment status for (a) ages 35-54, and (b) other ages on the odds of attending churches at least once a month.

##### (1)
We exponentiatate the coefficients of the logistic regression results. For the race dummy (`RACE_D`) variable, we find that it doubles the odds (`r odds["race_d"]`) of attending churches at least once a month.

##### (2)
###### (a)
For the employment status variable (`INT`) we find that it nearly halves the odds of attending church at least once a month (`r odds["int"]`).

###### (b)
For the effect of employment status (`WD`) we find that it increases the odds of attending church at least once a month by ~ 44% (`r odds["wd"]`)


### 3. Using the cross-classification of `LGT_P10` (which is ten times of the logit of the propensity score rounded to an integer value) and `ATTDDMY`, identify (a) the area of common support by the values of the `LGT-P10`, and (b) the number of sample subjects outside the common area of support.

```{r 3}
library(modelr)
logit2prob <- function(x){
  (exp(x) / (1 + exp(x)))
}

dat <- dat %>%
  add_predictions(q_1_logit) %>%
  mutate(prob = logit2prob(pred),
         lgt_p = log(prob / (1 - prob)),
         lgt_p10 = floor(lgt_p * 10 + .5))


dat %>%
  ggplot(aes(x = lgt_p10)) +
  geom_histogram(bins = 10) + 
  facet_wrap(~atddmy) +
  xlab("LGT-P10")


support_table <- dat %>%
  group_by(atddmy) %>%
  summarise(count = n(), min_lgt_p10 = min(lgt_p10), max_lgt_p10 = max(lgt_p10))


library(knitr)
kable(support_table)
```

We see that the common support, using the values of `LGT-P10` includes `r support_table$count[1] + support_table$count[2]` sample subjects, while the number of sample subjects outside of the common area of support is 0.

#### 4.  Define strata as follows using the `LGT_P10`.
(1) [-5, 3], (2) [-2, -1], (3) 0, (4) 1, (5) 2, (6) 3, (7) [4-6], (8) [7, 9], (9) [10, 14]

```{r 4}
dat <- dat %>%
  mutate(strata = case_when(
           (lgt_p10 >= -5 & lgt_p10 <= -3) ~ 1,
           (lgt_p10 >= -2 & lgt_p10 <= -1) ~ 2,
           (lgt_p10 == 0) ~ 3,
           (lgt_p10 == 1) ~ 4,
           lgt_p10 == 2 ~ 5,
           lgt_p10 == 3 ~ 6,
           (lgt_p10 >= 4 & lgt_p10 <= 6) ~ 7,
           (lgt_p10 >= 7 & lgt_p10 <= 9) ~ 8,
           (lgt_p10 >= 10 & lgt_p10 <= 14) ~ 9)) %>%
  na.omit() # Take off values with LGT_P10 outside of given range
```

#### 5. Calculate the mean of the logit of propensity score (`LOGIT_P`) by strata for each category of the treatment variable.

```{r 5}
q_5_table <- dat %>%
  group_by(strata, atddmy) %>%
  summarise(mean_logit_prop = mean(lgt_p))


strata_groups_2 <- dat %>%
  group_by(strata) %>%
  t.test(lgt_p ~ atddmy, data = .)


kable(q_5_table)
```

#### 6. Test the significance of (1) the difference in the logit between the treatment group and the control group for each stratum, and (2) estimate the average difference in the logit weighted by the proportion of samples in each stratum, and its standard error. Confirm that no significant differences in the logit exist.


```{r}
# (1)
strata_groups <- seq(1, 9) %>% 
  map(~ filter(dat, strata == .)) %>%
  map(~ t.test(lgt_p ~ atddmy, data = .)) %>%
  map_dbl(~ .$p.value)

# (2)
# See Session 3 slide 18 bottom half
ns <- dat %>%
  group_by(strata, atddmy) %>%
  summarise(n = n())
ns_treat <- ns %>%
  filter(as.integer(atddmy) == 2) %>%
  pull(n)
ns_control <- ns %>%
  filter(as.integer(atddmy) == 1) %>%
  pull(n)

sig_2s <- dat %>%
  group_by(strata, atddmy) %>%
  summarise(var = var(lgt_p))
sig_2s_treat <- sig_2s %>%
  filter(as.integer(atddmy) == 2) %>%
  pull(var)
sig_2s_control <- sig_2s %>%
  filter(as.integer(atddmy) == 1) %>%
  pull(var)

se_s <- sqrt(sig_2s_treat / ns_treat + sig_2s_control / ns_control)
  

p_s <- dat %>%
  count(strata) %>%
  mutate(p_strata = n / sum(n)) %>%
  pull(p_strata)

xs_bars <- dat %>%
    group_by(strata, atddmy) %>%
    summarise(lgt_p_mean = mean(lgt_p))
xs_bar_treat <- xs_bars %>%
  filter(as.integer(atddmy) == 2) %>%
  pull(lgt_p_mean)
xs_bar_control <- xs_bars %>%
  filter(as.integer(atddmy) == 1) %>%
  pull(lgt_p_mean)
  
  
weighted_test_stat <- sum((xs_bar_treat - xs_bar_control) * p_s)
weighted_se <- sqrt(sum((se_s ^ 2) * p_s))

ci <- c(weighted_test_stat - 2 * weighted_se, weighted_test_stat + 2 * weighted_se)
names(ci) <- c("Lower Confidence Bound (95%)", "Upper Confidence Bound (95%)")
print("P-values per strata for (1)")
names(strata_groups) <- paste("Strata: ", 1:9)
kable(strata_groups)

print("~95% confidence bound for (2)")
kable(ci)
```

Looking at each strata p-values (1) and the CI (2) we can confirm that no significant differences in the logit exist.
 
#### 7. Test the conditional independence between `ATDDMY` and each of the four variables (`EDUC5`, `RACE_D`, `AGE10C`, and `WD`) controlling for strata.

```{r 7, message=F}
library(bnlearn)
library(rlang)

ci_vars <- c("educ5", "race_d", "age10c", "wd")

ci_results <- ci_vars %>%
  map(~ mantelhaen.test(dat$atddmy, pull(dat, .), dat$strata)) %>%
  map_dbl(~ .$p.value)

names(ci_results) <- paste("atddmy ~", ci_vars)
kable(ci_results)
```

P-values seem to reassure us that there is conditional independence.


#### 8. Calculate the mean of the outcome variable (`TVHOURS`) for each stratum.

```{r 8}
kable(dat %>%
  group_by(strata) %>%
  summarise(mean_tvhours = mean(tvhours)))
```

#### 9. Calculate the estimate for the ATE and its standard error, and conclude whether church attendance significantly affects the hours of watching TV or not.

```{r 9}
treat_table <- dat %>%
  filter(as.integer(atddmy) == 2) %>%
  group_by(strata) %>%
  summarise(treat_mean_tv = mean(tvhours), 
            treat_sd_tv = sd(tvhours),
            treat_n = n(),
            treat_se = treat_sd_tv / sqrt(treat_n))

control_table <- dat %>%
  filter(as.integer(atddmy) == 1) %>%
  group_by(strata) %>%
  summarise(cont_mean_tv = mean(tvhours), 
            cont_sd_tv = sd(tvhours),
            cont_n = n(),
            cont_se = cont_sd_tv / sqrt(cont_n))

full_table <- bind_cols(control_table, treat_table) %>%
  mutate(mean_dif = treat_mean_tv - cont_mean_tv)

print("ATE:")
(ate <- weighted.mean(full_table$mean_dif, w = p_s))
print("SE:")
(se <- sd(full_table$mean_dif * p_s) / 3)
print("CI:")
(ci <- c(ate - 1.96 * se, ate + 1.96 * se))
```

Looking at the confidence interval we can conclude that church attendance does significantly affect the hours of watching TV (lowers them).

#### 10. Using the estimate of the propensity score, (1) create weights for `ATDDMY=1` and those for `ATDDMY=0` by the IPT formula for the ATE.

```{r 10}
p_treat <- nrow(dat %>% filter(as.integer(atddmy) == 2)) / nrow(dat)
p_control <- nrow(dat %>% filter(as.integer(atddmy) == 1)) / nrow(dat)

dat <- dat %>%
  mutate(treat_weight = p_treat / prob,
         cont_weight = p_control / (1 - prob))
```

#### 11. Check whether the sum of weighted frequencies for each of `ATDDMY`'s states is close to the number of samples for each state of `ATDDMY`. What can you tell from this diagnosis?

```{r 11}
weight_check <- dat %>%
  group_by(atddmy) %>%
  summarise(n = n(),
            treat_weighted_freq = sum(treat_weight),
            cont_weighted_freq = sum(cont_weight))

display_table <- weight_check[,c(1,2)] %>%
  mutate(weighted_frequency = c(weight_check$cont_weighted_freq[1], 
                                weight_check$treat_weighted_freq[2]),
         abs_dif = abs(weighted_frequency - n),
         abs_pct_dif = 100 *  (abs_dif / n))

kable(display_table)
```

These things are close, and thus the "convergence" test is passed and we can be assured that:
$ATE=\widehat{E}(Y_1) - \widehat{E}(Y_0)$.

#### 12. Adjust the IPT weights to have the average weight to be `1.0` for each state of the treatment variable.

```{r 12}
dat <- dat %>%
  mutate(adj_treat_weight = treat_weight + (1 - mean(dat$treat_weight)),
         adj_cont_weight = cont_weight + (1 - mean(dat$cont_weight))
         )
```


#### 13. Calculate the correlation between `ATDDMY` and the logit of the propensity score using the IPT weights. Confirm that correlation is nearly 0.

```{r 13}
# Each observation should have the appropriate ("actual") weight applied to it,
# a control # weight if atddmy is 1 (0 - church less than once a month), and a 
# treatment weight if atddmy is 2 (1 - church at least once a month).
dat <- dat %>%
  mutate(actual_adj_weight = ifelse(as.integer(atddmy) == 2, adj_treat_weight, 
                             adj_cont_weight),
         actual_weight = ifelse(as.integer(atddmy) == 2, treat_weight, 
                                cont_weight),
         adj_weight_tv_hours = actual_adj_weight * tvhours,
         weight_tv_hours = actual_weight * tvhours
  )

display_cor <- cor(dat$lgt_p, dat$actual_adj_weight)
names(display_cor) <- "Correlation between logit of propensity and the IPT weights"
(display_cor)
```
Looks close to 0 to me.

#### 14. Test the statistical independence between each of the four covariates and `ATDDMY` with the IPT-weighted data

```{r 14}
ci_vars <- c("educ5", "race_d", "age10c", "wd")

library(cobalt)
covs0 <- subset(dat, select = c(educ5, race_d, age10c, wd))
# Documentation for this "cobalt::cobalt_A0_basic_use" claims that leading
# methodologists claim that p-values are misleading for covariate balance, and
# hence they are omitted.
(results <- bal.tab(covs0, treat = dat$atddmy, weights = dat$actual_adj_weight,
                   method = "weighting", estimand = "ATE"))
```

This looks good (not statistically dependent), the R library we're using says that "leading methodologists" discourage hypothesis tests, and so it completely omits p-values in favor of summary statistics.


#### 15. Conduct the T-test for the difference in the weighted mean of `TVHOURS` between the two states of the treatment variable (`ATDDMY`), give estimate for the treatment effect, its standard error, and conclude. Assume a distinct variance of `Y` for each state of the treatment variable.

```{r 15}
(t_test_result <- t.test(adj_weight_tv_hours ~ atddmy, data = dat))

print("Estimate for treatment effect:")
(dif <- as.numeric(t_test_result$estimate[1] - t_test_result$estimate[2]))
print("SE:")
(se <- as.numeric((t_test_result$conf.int[1] - dif) / -1.96))

print("CI:")
(as.numeric(t_test_result$conf.int))
```

We conclude that `ATDDMY` significantly affects the hours of watching TV.

#### 16. Apply the following two linear regression models using `TVHOURS` as the dependent variables. (1) regression model with `ATDDMY` as the only explanator variable wit the IPT weights, and (2) regression model with `ATDDMY` and the control variables (use dummy variables for the education and age variables) as the explanatory variables with the IPT weights.

```{r 16}
lm1 <- lm(adj_weight_tv_hours ~ atddmy, data = dat)
lm2 <- lm(adj_weight_tv_hours ~ atddmy + educ5 + race_d + age10c + wd, data = dat,
          weights = dat$actual_adj_weight)

(lm1s <- summary(lm1))
```

#### 17. For the results from 16, answer the following.

#####  (a) Confirm that the treatment effect in the model of (1) is the same as the one we obtained in (15), but the standard error slightly differs. Why does it differ?

The estimate of the treatment effect from (15) is `r dif` and the treatment effect for the model of (1) is `r lm1s$coefficients[2, 1]`.

The standard error from (15) is `r se`, and the standard error from (1) is `r lm1s$coefficients[2,2]`.

The standard errors are different because the t.test has fewer degress of freedom than the linear regression this is because in 15 we assume a distinct variance of `Y` for each of the 2 states of the treatment variable `ATDDMY`, thus there is less confidence in our estimate, so the standard error is higher.

#####   (b) What is the purpose of applying model (2)? Why should we expect an estimate of the treatment effect that is very close to the result of model (1)?

```{r 17b}
(lm2s <- summary(lm2))
```

We apply model (2) to ensure that our IPTW procedure to adjust for the effect of certain covariates on our treatment variable (`ATDDMY`) was successful. We can see that IPTW worked well because the coefficients for `ATDDMY` are quite close between models (1) and (2).


```{r extra, include=F, eval=F}

treat_table <- dat %>%
  filter(as.integer(atddmy) == 2) %>%
  group_by(strata) %>%
  summarise(treat_mean_tv = mean(tvhours * actual_adj_weight), 
            treat_sd_tv = sd(tvhours * actual_adj_weight),
            treat_n = sum(actual_adj_weight),
            treat_se = treat_sd_tv / sqrt(treat_n))

control_table <- dat %>%
  filter(as.integer(atddmy) == 1) %>%
  group_by(strata) %>%
  summarise(cont_mean_tv = mean(tvhours * actual_adj_weight), 
            cont_sd_tv = sd(tvhours * actual_adj_weight),
            cont_n = sum(actual_adj_weight),
            cont_se = cont_sd_tv / sqrt(cont_n))

full_table_iptw <- bind_cols(control_table, treat_table) %>%
  mutate(mean_dif = treat_mean_tv - cont_mean_tv)

ate_iptw <- mean(full_table_iptw$mean_dif)
se_iptw <- sd(full_table_iptw$mean_dif) / 3
```